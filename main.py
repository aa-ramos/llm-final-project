import gradio as gr
from langchain.chains import LLMChain, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from vector_store import get_vector_store_global, get_vector_store_temp
from prompt_builder import process_user_query_two_stage, get_summary_measures_prompt

# Vetores globais
vector_store = get_vector_store_global()
retriever_global = vector_store.as_retriever()

# Vetor tempor√°rio da FINE
retriever_temporario = None

# Mem√≥rias separadas
memory_geral = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
memory_fine = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

def handle_upload(file):
    global retriever_temporario
    if file is None:
        return "‚ö†Ô∏è Nenhum ficheiro carregado."
    try:
        if not file.name.lower().endswith('.pdf'):
            return "‚ùå Por favor, carrega apenas ficheiros PDF."
        
        retriever_temporario = get_vector_store_temp(file.name)
        return "‚úÖ Documento carregado e pronto para an√°lise!"
    except FileNotFoundError:
        retriever_temporario = None
        return "‚ùå O ficheiro n√£o foi encontrado. Por favor, tenta novamente."
    except PermissionError:
        retriever_temporario = None
        return "‚ùå Erro de permiss√£o ao aceder ao ficheiro. Por favor, verifica as permiss√µes."
    except Exception as e:
        retriever_temporario = None
        return f"‚ùå Ocorreu um erro ao processar o ficheiro: {str(e)}"

def limpar_pdf():
    global retriever_temporario
    retriever_temporario = None
    return "‚ùå O documento foi eliminado."

def resumir_medidas_governo():
    global vector_store
    all_docs = vector_store.get()["documents"]
    metadados = vector_store.get()["metadatas"]

    # Filtra por ficheiro espec√≠fico
    chunks_apoios = [
        doc for doc, meta in zip(all_docs, metadados)
        if meta.get("source_file") == "medidas_do_governo.md"
    ]

    if not chunks_apoios:
        return "‚ùå N√£o foram encontrados conte√∫dos do ficheiro 'medidas_do_governo.md'."

    context = "\n".join(chunks_apoios)

    # Gera o resumo com LLM
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    summary_chain = LLMChain(llm=llm, prompt=get_summary_measures_prompt())
    resumo = summary_chain.invoke({"context": context})

    return resumo["text"]

def echo_geral(message, history):
    global retriever_global
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

    prompt_template = process_user_query_two_stage(
        user_input=message, 
        vector_store=retriever_global, 
        llm=llm, 
        use_imt_specific="imt" in message.lower()
    )

    if isinstance(prompt_template, str):
        return prompt_template

    conv_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever_global,
        memory=memory_geral,
        combine_docs_chain_kwargs={"prompt": prompt_template},
        return_source_documents=False
    )

    response = conv_chain({"question": message})
    return response["answer"]

def echo_fine(message, history):
    global retriever_temporario

    if not retriever_temporario:
        return "‚ö†Ô∏è Nenhum documento FINE carregado. Por favor, faz upload antes de continuar."

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    # Get the appropriate prompt template
    prompt_template = process_user_query_two_stage(
        user_input=message, 
        vector_store=retriever_temporario, 
        llm=llm
    )

    # If we got a string response (like out of scope), return it directly
    if isinstance(prompt_template, str):
        return prompt_template

    retriever = retriever_temporario.as_retriever()
    conv_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory_fine,
        combine_docs_chain_kwargs={"prompt": prompt_template},
        return_source_documents=False
    )
    response = conv_chain({"question": message})
    return response["answer"]

# Fun√ß√£o para limpar mem√≥ria + chatbot
# def reset_chat_history(memory, chatbot):
#     memory.clear()
#     return []


with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## üè° Descomplica o Cr√©dito √† Habita√ß√£o")
    gr.Markdown("üëã Ol√°! Bem-vindo ao teu assistente de Cr√©dito √† Habita√ß√£o! Em que te posso ajudar?")
    
    with gr.Tabs() as tabs:
        with gr.Tab("üí¨ Geral"):
            with gr.Row():
                # Coluna do chat (mais larga)
                with gr.Column(scale=8, min_width=500):
                    chatbot_geral = gr.Chatbot(height=650, show_copy_button=True)
                    gr.ChatInterface(fn=echo_geral, type="messages", chatbot=chatbot_geral, save_history=True)
                    # botao_reset_geral = gr.Button("üîÑ Reiniciar conversa")
                    # botao_reset_geral.click(
                    #     fn=lambda: reset_chat_history(memory_geral, chatbot_geral),
                    #     outputs=chatbot_geral
                    # )

                # Coluna do resumo (mais estreita)
                with gr.Column(scale=5, min_width=400):
                    gr.Markdown("### ‚ú® Funcionalidade Extra")
                    gr.Markdown("Aqui tens um pequeno resumo sobre os **Apoios do Governo** atualmente em vigor. Espero que te seja √∫til!")
                    resumo_button = gr.Button("üîç Gerar Resumo")
                    resumo_output = gr.Markdown(value="", max_height=500)
                    resumo_button.click(fn=resumir_medidas_governo, outputs=resumo_output)


        with gr.Tab("üìÑ FINE"):
            gr.Markdown("Carrega o teu documento **FINE** para uma an√°lise personalizada.")
            file_input = gr.File(label="Seleciona o ficheiro PDF:", file_types=[".pdf"])
            
            upload_button = gr.Button("üì§ Processar documento")
            upload_output = gr.Markdown()

            # Quando carrega ficheiro e clica no bot√£o
            upload_button.click(fn=handle_upload, inputs=file_input, outputs=upload_output)

            # Quando remove o ficheiro (a√ß√£o da cruz do gr.File)
            file_input.clear(fn=limpar_pdf, outputs=upload_output)
            
            
            # with gr.Row(equal_height=True):
            #     upload_button = gr.Button("üì§ Processar documento")
            #     limpar_button = gr.Button("üóëÔ∏è Limpar documento")
            # upload_output = gr.Markdown()

            # upload_button.click(fn=handle_upload, inputs=file_input, outputs=upload_output)
            # limpar_button.click(fn=limpar_pdf, outputs=upload_output)

            chatbot_fine = gr.Chatbot()
            gr.ChatInterface(fn=echo_fine, type="messages", chatbot=chatbot_fine)
            # botao_reset_fine = gr.Button("üîÑ Reiniciar conversa")
            # botao_reset_fine.click(
            #     fn=lambda: reset_chat_history(memory_fine, chatbot_fine),
            #     outputs=chatbot_fine
            # )
            
    gr.HTML(
    """
    <div style='background-color:#f5f5f5; padding:10px; border-radius:8px;
                border:1px solid #ddd; text-align:center; font-size:14px; color:#444;'>
        ‚ö†Ô∏è Este projeto destina-se apenas a fins informativos e educacionais, n√£o constituindo aconselhamento financeiro.
        Todas as decis√µes financeiras s√£o da inteira responsabilidade do utilizador.
    </div>
    """
    )

if __name__ == "__main__":
    demo.launch()